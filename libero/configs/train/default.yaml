# training
n_epochs: 50
eval_every: 10
batch_size: 32
num_workers: 4
grad_clip: 100.
loss_scale: 1.0

# resume training
resume: false
resume_path: ""
debug: false

use_augmentation: true
low_dim_aug_std: 1e-5

defaults:
    - optimizer@optimizer: adam_w.yaml
    - scheduler@scheduler: cosine_annealing.yaml
